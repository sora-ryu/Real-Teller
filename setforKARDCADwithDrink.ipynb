{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed to display the images.\n",
    "%matplotlib inline\n",
    "\n",
    "# This is needed since the notebook is stored in the object_detection folder.\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import label_map_util\n",
    "\n",
    "from utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What model to download.\n",
    "MODEL_NAME = 'ssd_mobilenet_v1_coco_2017_11_17'\n",
    "MODEL_FILE = MODEL_NAME + '.tar.gz'\n",
    "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
    "\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')\n",
    "\n",
    "NUM_CLASSES = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "opener = urllib.request.URLopener()\n",
    "opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
    "tar_file = tarfile.open(MODEL_FILE)\n",
    "for file in tar_file.getmembers():\n",
    "  file_name = os.path.basename(file.name)\n",
    "  if 'frozen_inference_graph.pb' in file_name:\n",
    "    tar_file.extract(file, os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "  od_graph_def = tf.GraphDef()\n",
    "  with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "    serialized_graph = fid.read()\n",
    "    od_graph_def.ParseFromString(serialized_graph)\n",
    "    tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1011 09:12:21.547904  8580 deprecation_wrapper.py:119] From C:\\Users\\IVPL-D11\\models\\research\\object_detection\\utils\\label_map_util.py:132: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pykinect2 import PyKinectV2\n",
    "from pykinect2.PyKinectV2 import *\n",
    "from pykinect2 import PyKinectRuntime\n",
    "from acquisitionKinect import AcquisitionKinect\n",
    "from frame import Frame as Frame\n",
    "import numpy as np\n",
    "import cv2\n",
    "import ctypes\n",
    "import _ctypes\n",
    "import sys\n",
    "import face_recognition\n",
    "import os\n",
    "from scipy import io\n",
    "import math\n",
    "from gtts import gTTS\n",
    "import pyttsx3\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "from textblob import TextBlob\n",
    "engine = pyttsx3.init()\n",
    "engine.setProperty('rate', 145)     # setting up new voice rate\n",
    "engine.setProperty('volume',1.0)    # setting up volume level  between 0 and 1\n",
    "voices = engine.getProperty('voices')       #getting details of current voice\n",
    "engine.setProperty('voice', voices[0].id)  #changing index, changes voices.\n",
    "\n",
    "#Load Basic Setting for Kinect\n",
    "Kinect = AcquisitionKinect()\n",
    "frame = Frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadingforAction():\n",
    "    filename = 'KARDCADwithDrink3.mat'\n",
    "    AM = io.loadmat(filename, mat_dtype = False)\n",
    "\n",
    "    CiM_o = AM['Cim_out']\n",
    "\n",
    "    CiM = np.array([])\n",
    "    for i in range(25):\n",
    "        CiM = np.concatenate((CiM,np.array(CiM_o)['value'][0,0][0][int(i)][0]))\n",
    "    CiM = CiM.reshape((25, D))\n",
    "\n",
    "    iMaxis_o = AM['IMaxis_out']\n",
    "    iMaxis = np.array([])\n",
    "    for i in range(3):\n",
    "        iMaxis = np.concatenate((iMaxis,np.array(iMaxis_o)['value'][0,0][0][int(i)][0]))\n",
    "    iMaxis = iMaxis.reshape((3, D))\n",
    "    \n",
    "    iMjoints_o = AM['IMjoints_out']\n",
    "    iMjoints = np.array([])\n",
    "    for i in range(14):\n",
    "        iMjoints = np.concatenate((iMjoints,np.array(iMjoints_o)['value'][0,0][0][int(i)][0]))\n",
    "    iMjoints = iMjoints.reshape((14, D))\n",
    "    \n",
    "    AM_o = AM['AM2str']\n",
    "    AMM = np.array([])\n",
    "    for i in range(4):\n",
    "        AMM = np.concatenate((AMM,np.array(AM_o)['value'][0,0][0][int(i)][0]))\n",
    "    AMM = AMM.reshape((4, D))\n",
    "    \n",
    "    return CiM, iMaxis, iMjoints, AMM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosAngle (u, v):\n",
    "    cosAngle = np.dot(u,v)/(np.linalg.norm(u)*np.linalg.norm(v))\n",
    "    return cosAngle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarizeHV (v):\n",
    "    threshold = 0\n",
    "    for i in range(len(v)):\n",
    "        if v[i] > threshold:\n",
    "            v[i] = 1\n",
    "        else:\n",
    "            v[i] = -1\n",
    "    return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(data, bins):\n",
    "    split = np.array_split(np.sort(data), bins)\n",
    "    cutoffs = [x[-1] for x in split]\n",
    "    cutoffs = cutoffs[:-1]\n",
    "    discrete = np.digitize(data, cutoffs, right=True)\n",
    "    return discrete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hdcResult(AM,M0,CiM,iMjoints,iMaxis,MAXL,compareAngle):\n",
    "    #TP = np.zeros((1,17))\n",
    "    #FN = np.zeros((1,17))\n",
    "    #FP = np.zeros((1,17))\n",
    "    predictedLabel = -1\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    print(\"in\")\n",
    "    #for j in range (3):\n",
    "    for i in range(1, 5):\n",
    "        if(i == 1 or i == 2 or i==3 or i == 4 ):\n",
    "            tmp = np.zeros(D)\n",
    "            M = discretize(M0, MAXL)\n",
    "            M = M.reshape((30, 42))\n",
    "            for k in range( len(M)):\n",
    "                    \n",
    "                for n in range(len(M[0])):\n",
    "                       \n",
    "                    jointHDC = iMjoints[math.floor(n/3)]\n",
    "                    axisHDC = iMaxis[n%3]\n",
    "                     \n",
    "                    valueHDC = CiM[M[k][n]]\n",
    "                       #\n",
    "                    tmp += ((jointHDC*axisHDC) * valueHDC)\n",
    "                maxAngle = compareAngle\n",
    "                for b in range (1,5):\n",
    "                        \n",
    "                    if(b == 1 or b == 2 or b==3 or b ==4 ):\n",
    "                        angle = cosAngle (AM[b-1], binarizeHV(tmp))\n",
    "                            \n",
    "                        if (angle > maxAngle):\n",
    "                            maxAngle = angle\n",
    "                            predictedLabel = b\n",
    "                    #if predictedLabel == i:\n",
    "                        #TP(:,i) = TP(:,i)+1\n",
    "                    #else:\n",
    "                        #FN(:,i) = FN(:,i)+1;\n",
    "                        #FP(:,predictedLabel) = FP(:,predictedLabel)+1\n",
    "   #print(\"predictLabel:\", predictedLabel)\n",
    "\n",
    "    #precision = TP./(TP+FP)\n",
    "    #recall = TP./(TP+FN)\n",
    "    return predictedLabel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# setting for Face Recognition\n",
    "known_face_encodings = []\n",
    "known_face_names = []\n",
    "\n",
    "# Load sample pictures and learn how to recognize it.\n",
    "dirname = 'face_reco/knowns'\n",
    "files = os.listdir(dirname)\n",
    "for filename in files:\n",
    "    name, ext = os.path.splitext(filename)\n",
    "    if ext == '.jpg':\n",
    "        known_face_names.append(name)\n",
    "        pathname = os.path.join(dirname, filename)\n",
    "        img = face_recognition.load_image_file(pathname)\n",
    "        face_encoding = face_recognition.face_encodings(img)[0]\n",
    "        known_face_encodings.append(face_encoding)\n",
    "\n",
    "# Initialize some variables\n",
    "face_locations = []\n",
    "face_encodings = []\n",
    "face_names = []\n",
    "process_this_frame = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['person', 6.760330578512397]]\n",
      "face:  []\n",
      "people:  []\n",
      "people_max:  -1\n",
      "index:  0\n",
      "detected object: \n",
      " [['person', 6.760330578512397]]\n",
      "find 0\n",
      "[['person', 16.363636363636363]]\n",
      "find 1\n",
      "[['person', 11.710743801652892]]\n",
      "find 2\n",
      "[['person', 34.85123966942149]]\n",
      "find 3\n",
      "[['person', 10.727272727272727]]\n",
      "find 4\n",
      "[['person', 9.355371900826446]]\n",
      "find 5\n",
      "[['person', 51.19834710743802]]\n",
      "find 6\n",
      "[['person', 23.206611570247933]]\n",
      "find 7\n",
      "[['person', 33.95867768595041]]\n",
      "find 8\n",
      "[['person', 45.578512396694215]]\n",
      "find 9\n",
      "[['person', 42.21487603305785]]\n",
      "find 10\n",
      "[['person', 43.28099173553719]]\n",
      "find 11\n",
      "[['person', 44.95867768595041]]\n",
      "find 12\n",
      "[['Eunjung', 46.65289256198347]]\n",
      "face:  ['Eunjung']\n",
      "people:  ['Eunjung']\n",
      "people_max:  0\n",
      "find 13\n",
      "[['Eunjung', 44.752066115702476]]\n",
      "find 14\n",
      "[['Eunjung', 43.752066115702476]]\n",
      "find 15\n",
      "[['Eunjung', 42.88429752066116]]\n",
      "find 16\n",
      "[['Eunjung', 43.18181818181818]]\n",
      "find 17\n",
      "[['Eunjung', 43.22314049586777]]\n",
      "find 18\n",
      "[['Eunjung', 43.082644628099175]]\n",
      "find 19\n",
      "[['Eunjung', 44.56198347107438]]\n",
      "find 20\n",
      "[['dog', 37.950413223140494], ['person', 48.22314049586777]]\n",
      "index:  1\n",
      "detected object: \n",
      " [['dog', 37.950413223140494], ['person', 48.22314049586777]]\n",
      "find 21\n",
      "[['person', 66.01652892561984]]\n",
      "find 22\n",
      "[['Eunjung', 77.97520661157024]]\n",
      "find 23\n",
      "[['Eunjung', 87.45454545454545]]\n",
      "find 24\n",
      "[['Eunjung', 87.0]]\n",
      "find 25\n",
      "[]\n",
      "find 26\n",
      "[]\n",
      "find 27\n",
      "[['Sora', 89.06611570247934]]\n",
      "find 28\n",
      "[]\n",
      "find 29\n",
      "in\n",
      "Action Detect----------------------------------------------------------\n",
      "sit\n",
      "-----------------------------------------------------------------------\n",
      "eng:  Eunjung is sitting behind dog.\n",
      "ko:  은정이 개 뒤에 앉아있다.\n",
      "[['Eunjung', 107.52066115702479]]\n",
      "face:  ['Eunjung']\n",
      "people:  ['Eunjung']\n",
      "people_max:  -1\n",
      "index:  0\n",
      "detected object: \n",
      " [['Eunjung', 107.52066115702479]]\n",
      "find 0\n",
      "[['person', 108.80165289256199]]\n",
      "find 1\n",
      "[['person', 108.9504132231405]]\n",
      "find 2\n",
      "[['person', 108.89256198347107]]\n",
      "find 3\n",
      "[['person', 108.99173553719008]]\n",
      "find 4\n",
      "[['person', 107.94214876033058]]\n",
      "find 5\n",
      "[['person', 108.48760330578513]]\n",
      "find 6\n",
      "[['person', 108.59504132231405]]\n",
      "find 7\n",
      "[['person', 108.8099173553719]]\n",
      "find 8\n",
      "[['person', 108.86776859504133]]\n",
      "find 9\n",
      "[['person', 108.8099173553719]]\n",
      "find 10\n",
      "[['person', 108.81818181818181]]\n",
      "find 11\n",
      "[['person', 108.93388429752066]]\n",
      "find 12\n",
      "[['person', 108.92561983471074]]\n",
      "find 13\n",
      "[['person', 108.9504132231405]]\n",
      "find 14\n",
      "[['person', 108.53719008264463]]\n",
      "find 15\n",
      "[['person', 108.90082644628099]]\n",
      "find 16\n",
      "[['person', 108.93388429752066]]\n",
      "find 17\n",
      "[['person', 108.73553719008264]]\n",
      "find 18\n",
      "[['person', 109.46280991735537]]\n",
      "find 19\n",
      "[['person', 109.2892561983471]]\n",
      "find 20\n",
      "[['person', 108.93388429752066]]\n",
      "find 21\n",
      "[['person', 108.85123966942149]]\n",
      "find 22\n",
      "[['person', 109.04132231404958]]\n",
      "find 23\n",
      "[['person', 109.31404958677686]]\n",
      "find 24\n",
      "[['person', 108.56198347107438]]\n",
      "find 25\n",
      "[['person', 108.88429752066116]]\n",
      "find 26\n",
      "[['person', 108.9090909090909]]\n",
      "find 27\n",
      "[['person', 109.0]]\n",
      "find 28\n",
      "[['person', 108.6198347107438]]\n",
      "find 29\n",
      "in\n",
      "Action Detect----------------------------------------------------------\n",
      "sit\n",
      "-----------------------------------------------------------------------\n",
      "eng:  Eunjung is sitting in front of Eunjung.\n",
      "ko:  은정은 은정 앞에 앉아있다.\n",
      "[['person', 7.702479338842975]]\n",
      "face:  []\n",
      "people:  []\n",
      "people_max:  -1\n",
      "index:  0\n",
      "detected object: \n",
      " [['person', 7.702479338842975]]\n",
      "find 0\n",
      "[['person', 7.0]]\n",
      "find 1\n",
      "[['person', 7.0]]\n",
      "find 2\n",
      "[['person', 7.024793388429752]]\n",
      "find 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-17c5659ca535>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    117\u001b[0m                     \u001b[0mimage_np\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m                     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m                     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m                     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m                     \u001b[0mcategory_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def make_sentence(depth, position, index, start, end):\n",
    "    objects = \"\"\n",
    "    objects += position + \" \"\n",
    "    for i in range(start, end):\n",
    "        objects += depth[i][0] + \" and \"\n",
    "\n",
    "                                    \n",
    "    objects += depth[end][0]\n",
    "\n",
    "    return objects\n",
    "\n",
    "\n",
    "# Functions for Action Recognition\n",
    "D = 7000\n",
    "numActivities = 3\n",
    "MAXL = 25\n",
    "compareAngle = 0.5\n",
    "feature = np.zeros((20,3))\n",
    "people = []\n",
    "i = 1\n",
    "framecheck = 0\n",
    "max_depth = -1 \n",
    "max_people = -1\n",
    "frameset = np.array([])\n",
    "norm_frameset = np.array([])\n",
    "CiM, iMaxis, iMjoints, AMM = loadingforAction()\n",
    "flag = 0\n",
    "index = -1\n",
    "framecheck = 0\n",
    "#Object Detection\n",
    "with detection_graph.as_default():\n",
    "  with tf.Session(graph=detection_graph) as sess:\n",
    "    while True:\n",
    "        peop = 0\n",
    "        situation=\"\"\n",
    "        # --- Getting frames and drawing\n",
    "        Kinect.get_frame(frame)\n",
    "        Kinect.get_color_frame()\n",
    "        image_np = Kinect._kinect.get_last_color_frame()\n",
    "        #image_np = Kinect._frameRGB\n",
    "        image_depth = Kinect._frameDepthQuantized \n",
    "        Skeleton_img = Kinect._frameSkeleton\n",
    "        image_np = np.reshape(image_np, (Kinect._kinect.color_frame_desc.Height, Kinect._kinect.color_frame_desc.Width, 4))\n",
    "        image_np = cv2.cvtColor(image_np, cv2.COLOR_RGBA2RGB)\n",
    "        show_img = image_np\n",
    "       # show_img = image_np[ 200:1020, 350:1780]\n",
    "        show_img = cv2.resize(show_img, (512,424))\n",
    "        \n",
    "        #image_np = cv2.cvtColor(image_np, cv2.COLOR_RGBA2RGB)\n",
    "        #image_np = image_np[ 200:1020, 350:1780]\n",
    "        #image_np = cv2.resize(image_np, (512,424))\n",
    "        rgb_small_frame = cv2.resize(image_np, (0, 0), fx=0.25, fy=0.25)\n",
    "\n",
    "# Face recognition\n",
    "        if process_this_frame:\n",
    "            # Find all the faces and face encodings in the current frame of video\n",
    "            face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "            face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "\n",
    "            face_names = []\n",
    "            for face_encoding in face_encodings:\n",
    "                distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "                min_value = min(distances)\n",
    "\n",
    "\n",
    "                name = \"Unknown person\"\n",
    "                if min_value < 0.5:\n",
    "                    index = np.argmin(distances)\n",
    "                    name = known_face_names[index]\n",
    "\n",
    "                face_names.append(name)\n",
    "\n",
    "        process_this_frame = not process_this_frame\n",
    "\n",
    "       \n",
    "        for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "            # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n",
    "            top *= 4\n",
    "            right *= 4\n",
    "            bottom *= 4\n",
    "            left *= 4\n",
    "\n",
    "            # Draw a box around the face\n",
    "            cv2.rectangle(image_np, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "            # Draw a label with a name below the face\n",
    "            cv2.rectangle(image_np, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "            \n",
    "            font = cv2.FONT_HERSHEY_DUPLEX\n",
    "            cv2.putText(image_np, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "            \n",
    "#---------------- Object Detection\n",
    "\n",
    "        image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "        detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "        detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "        detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "        num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "\n",
    "        image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    " \n",
    "        (boxes, scores, classes, num) = sess.run(\n",
    "            [detection_boxes, detection_scores, detection_classes, num_detections],\n",
    "            feed_dict={image_tensor: image_np_expanded})\n",
    "\n",
    "        vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "            image_np,\n",
    "            np.squeeze(boxes),\n",
    "            np.squeeze(classes).astype(np.int32),\n",
    "            np.squeeze(scores),\n",
    "            category_index,\n",
    "            use_normalized_coordinates=True,\n",
    "            line_thickness=8)\n",
    "\n",
    "\n",
    "        coordinates = vis_util.return_coordinates(\n",
    "                    image_np,\n",
    "                    np.squeeze(boxes),\n",
    "                    np.squeeze(classes).astype(np.int32),\n",
    "                    np.squeeze(scores),\n",
    "                    category_index,\n",
    "                    use_normalized_coordinates=True,\n",
    "                    line_thickness=8,\n",
    "                    min_score_thresh=0.5)\n",
    "        \n",
    "        if coordinates is not None:\n",
    "            depth_arr = [[0 for x in range(2)] for y in range(len(coordinates))]\n",
    "            real_depth_arr = [[0 for x in range(2)] for y in range(len(coordinates))]\n",
    "            count = 0\n",
    "            for i in range(len(coordinates)):\n",
    "                if coordinates[i][0] == 'person':\n",
    "                    flag = 1\n",
    "                    if count < len(face_names):\n",
    "                        coordinates[i][0] = face_names[count]\n",
    "                        count += 1\n",
    "                        \n",
    "            #print(coordinates)\n",
    "            xpoint_for_depth=[]\n",
    "            ypoint_for_depth=[]\n",
    "            \n",
    "            #print(\"object 개수:\", len(coordinates))\n",
    "            for i in range(len(coordinates)):\n",
    "                ypoint_for_depth.append(int((coordinates[i][1]+coordinates[i][2])/2))\n",
    "                xpoint_for_depth.append(int((coordinates[i][3]+coordinates[i][4])/2))\n",
    "\n",
    "            #print(\"ypoint_for_depth:\", ypoint_for_depth)\n",
    "            #print(\"xpoint_for_depth:\", xpoint_for_depth)\n",
    "            m = 0\n",
    "            for i in range(len(coordinates)):\n",
    "                if(coordinates[i][0]!=\"N/A\" and coordinates[i][0] not in depth_arr):\n",
    "                    depth_arr[m][0] = coordinates[i][0]\n",
    "                    depth_arr[m][1] = 0\n",
    "                    for x in range (-5, 6):\n",
    "                        for y in range(-5, 6):\n",
    "                            depth_arr[m][1] += image_depth[int((ypoint_for_depth[i]+x) * 424 / 1080)][int((xpoint_for_depth[i]+y) * 512 / 1920)]\n",
    "                            show_img[int((ypoint_for_depth[i]+x) * 424 / 1080)][int((xpoint_for_depth[i]+y) * 512 / 1920)] = [255, 0,95]\n",
    "                    depth_arr[m][1] /= 121\n",
    "                    m+=1\n",
    "\n",
    "            # Compare each objects' depth. (Sorting: close -> far)\n",
    "            depth_arr.sort(key=lambda x:x[1])\n",
    "            print(depth_arr)\n",
    "       \n",
    "               # print(coordinates[i][0], \"'s depth: \", Pixel_Depth)\n",
    "        image_np = cv2.resize(image_np,  (640, 480))\n",
    "        cv2.imshow(\"ee\", image_np)  \n",
    "        \n",
    "        cv2.imshow(\"depthh\", image_depth)\n",
    "        #cv2.imshow('KINECT Color', image_np)\n",
    "        \n",
    "        # Rule of Generating Sentence\n",
    "        if(len(face_names)> max_people):\n",
    "            print(\"face: \", face_names)\n",
    "            for i in range(len(face_names)): \n",
    "                if face_names[i] not in people:\n",
    "                    people.append( face_names[i])\n",
    "            print(\"people: \", people)\n",
    "            print(\"people_max: \", max_people)\n",
    "            max_people = len(people)\n",
    "            \n",
    "        if(len(depth_arr)> max_depth):\n",
    "            tmp_situation = \"\"\n",
    "            for i in range(len(depth_arr)):\n",
    "                if(depth_arr[i][0] in face_names or depth_arr[i][0]== \"person\"):\n",
    "                    index = i\n",
    "            print(\"index: \", index)\n",
    "\n",
    "            print(\"detected object: \\n\", depth_arr)                        \n",
    "            if index==0:\n",
    "                tmp_situation += make_sentence(depth_arr, \"in front of\", index, 1, len(depth_arr)-1 )\n",
    "            elif index == len(depth_arr)-1:\n",
    "                tmp_situation += make_sentence(depth_arr, \"behind\", index, 0, len(depth_arr)-2)\n",
    "            elif index > 0 and index <len(depth_arr)-1 :\n",
    "                tmp_situation += make_sentence(depth_arr, \"behind\", index, 0, index-1 )\n",
    "                tmp_situation += \" and \"\n",
    "                tmp_situation += make_sentence(depth_arr, \"in front of\", index, index + 1, len(depth_arr)-1 )\n",
    "            max_depth = len(depth_arr)\n",
    "#Action Recognition\n",
    "       \n",
    "        \n",
    "        if Skeleton_img is not None:\n",
    "\n",
    "            \n",
    "            cv2.resize(Skeleton_img, (960, 540))\n",
    "            cv2.imshow(\"joint\", Skeleton_img)\n",
    "            print(\"find\", framecheck)            \n",
    "           \n",
    "            feature[1][0] = Kinect.joint_points3D[3].x\n",
    "            feature[1][1] = Kinect.joint_points3D[3].y\n",
    "            feature[1][2] = Kinect.joint_points3D[3].z\n",
    "            \n",
    "            feature[2][0] = Kinect.joint_points3D[2].x\n",
    "            feature[2][1] = Kinect.joint_points3D[2].y\n",
    "            feature[2][2] = Kinect.joint_points3D[2].z\n",
    "            \n",
    "            feature[3][0] = Kinect.joint_points3D[8].x\n",
    "            feature[3][1] = Kinect.joint_points3D[8].y\n",
    "            feature[3][2] = Kinect.joint_points3D[8].z\n",
    "\n",
    "            feature[4][0] = Kinect.joint_points3D[9].x\n",
    "            feature[4][1] = Kinect.joint_points3D[9].y\n",
    "            feature[4][2] = Kinect.joint_points3D[9].z\n",
    "\n",
    "            feature[5][0] = Kinect.joint_points3D[11].x\n",
    "            feature[5][1] = Kinect.joint_points3D[11].y\n",
    "            feature[5][2] = Kinect.joint_points3D[11].z\n",
    "\n",
    "            feature[6][0] = Kinect.joint_points3D[4].x\n",
    "            feature[6][1] = Kinect.joint_points3D[4].y\n",
    "            feature[6][2] = Kinect.joint_points3D[4].z\n",
    "\n",
    "            feature[7][0] = Kinect.joint_points3D[5].x\n",
    "            feature[7][1] = Kinect.joint_points3D[5].y\n",
    "            feature[7][2] = Kinect.joint_points3D[5].z\n",
    "\n",
    "            feature[8][0] = Kinect.joint_points3D[7].x\n",
    "            feature[8][1] = Kinect.joint_points3D[7].y\n",
    "            feature[8][2] = Kinect.joint_points3D[7].z\n",
    "\n",
    "            feature[9][0] = Kinect.joint_points3D[1].x\n",
    "            feature[9][1] = Kinect.joint_points3D[1].y\n",
    "            feature[9][2] = Kinect.joint_points3D[1].z\n",
    "\n",
    "            feature[10][0] = Kinect.joint_points3D[16].x\n",
    "            feature[10][1] = Kinect.joint_points3D[16].y\n",
    "            feature[10][2] = Kinect.joint_points3D[16].z\n",
    "\n",
    "            feature[11][0] = Kinect.joint_points3D[17].x\n",
    "            feature[11][1] = Kinect.joint_points3D[17].y\n",
    "            feature[11][2] = Kinect.joint_points3D[17].z\n",
    "\n",
    "            feature[12][0] = Kinect.joint_points3D[19].x\n",
    "            feature[12][1] = Kinect.joint_points3D[19].y\n",
    "            feature[12][2] = Kinect.joint_points3D[19].z\n",
    "\n",
    "            feature[13][0] = Kinect.joint_points3D[12].x\n",
    "            feature[13][1] = Kinect.joint_points3D[12].y\n",
    "            feature[13][2] = Kinect.joint_points3D[12].z\n",
    "\n",
    "            feature[14][0] = Kinect.joint_points3D[13].x\n",
    "            feature[14][1] = Kinect.joint_points3D[13].y\n",
    "            feature[14][2] = Kinect.joint_points3D[13].z\n",
    "\n",
    "            feature[15][0] = Kinect.joint_points3D[15].x\n",
    "            feature[15][1] = Kinect.joint_points3D[15].y\n",
    "            feature[15][2] = Kinect.joint_points3D[15].z\n",
    "            tmp = []\n",
    "            for i in range (1, 16):\n",
    "                for j in range(3):\n",
    "                    tmp.append(feature[i][j])\n",
    "\n",
    "            frameset = np.concatenate((frameset, tmp), axis = 0)\n",
    "            framecheck+=1\n",
    "            if(framecheck == 30):\n",
    "                frameset = frameset.reshape((framecheck, 45))\n",
    "                for row in range(len(frameset)):\n",
    "                    \n",
    "                    torsox = float(frameset[row][24])\n",
    "                    torsoy = float(frameset[row][25])\n",
    "                    torsoz = float(frameset[row][26])\n",
    "                    neckx = float(frameset[row][3])\n",
    "                    necky = float(frameset[row][4])\n",
    "                    neckz = float(frameset[row][5])\n",
    "                    denom = math.sqrt(math.pow((neckx - torsox),2) + math.pow((necky - torsoy),2) + math.pow((neckz - torsoz),2))\n",
    "                    tmp = []\n",
    "                    i = 0\n",
    "                    while (i < 24):\n",
    "                        tmp.append((float(frameset[row][i]) - torsox)/denom)\n",
    "                        tmp.append((float(frameset[row][i+1]) - torsoy)/denom)\n",
    "                        tmp.append((float(frameset[row][i+2]) - torsoz)/denom)\n",
    "                        i+=3\n",
    "                    i = 27\n",
    "                    while (i < 45):\n",
    "                        tmp.append((float(frameset[row][i]) - torsox)/denom)\n",
    "                        tmp.append((float(frameset[row][i+1]) - torsoy)/denom)\n",
    "                        tmp.append((float(frameset[row][i+2]) - torsoz)/denom)\n",
    "                        i+=3\n",
    "                    norm_frameset = np.concatenate((norm_frameset, tmp), axis = 0)\n",
    "                    \n",
    "                #norm_frameset = norm_frameset.reshape((k, 42))\n",
    "               # CiM,iMjoints,iMaxis = initItemMemories (D, MAXL)\n",
    "                \n",
    "                predictedLabel = hdcResult(AMM,norm_frameset,CiM,iMjoints,iMaxis,MAXL,compareAngle)\n",
    "                print(\"Action Detect----------------------------------------------------------\")\n",
    "                if(predictedLabel == 1):\n",
    "                    action = \"standing\"\n",
    "                    print(\"still\")\n",
    "                elif (predictedLabel == 2 ):\n",
    "                    action = \"sitting\"\n",
    "                    print (\"sit\")\n",
    "                elif (predictedLabel == 3 ):\n",
    "                    action = \"walking\"\n",
    "                    print (\"walk\")\n",
    "                elif predictedLabel == 4 :\n",
    "                    action = \"drinking water\"\n",
    "                    print(\"drink\")\n",
    "                print(\"-----------------------------------------------------------------------\")\n",
    "               \n",
    "                tmp = []\n",
    "                frameset = np.array([])\n",
    "                norm_frameset = np.array([])\n",
    "                framecheck = 0\n",
    "                \n",
    "                            \n",
    "       \n",
    "                back_objects = \"\"\n",
    "                front_objects = \"\"\n",
    "                if(flag==1):\n",
    "                    if(max_people <2): # one person\n",
    "                        if(max_people ==1):\n",
    "                            situation += people[0] + \" is \"\n",
    "                        elif max_people <1:\n",
    "                            situation += \"Unknown person is \"\n",
    "                        situation += action + \" \"\n",
    "                        situation += tmp_situation\n",
    "                        #back_objects += situation\n",
    "                        #front_objects += situation\n",
    "                        \n",
    "                        \n",
    "                    elif (max_people>=2) : # more than two people\n",
    "                        for i in range(len(people)-1):\n",
    "                            situation += people[i] + \" and \"\n",
    "                        situation += people[len(people)-1]+ \" are near by \"\n",
    "                   # if \"person\" in Pixel_Depth_Dict.keys():\n",
    "\n",
    "                        situation += action\n",
    "                    situation += \".\"\n",
    "                    print(\"eng: \", situation)\n",
    "                    word = TextBlob(situation)\n",
    "                    fin_situation = str(word.translate(from_lang='en', to='ko'))\n",
    "                    engine.say(fin_situation)\n",
    "                    engine.runAndWait()\n",
    "                    print(\"ko: \", fin_situation)\n",
    "                    max_depth = -1\n",
    "                    max_people = -1\n",
    "                    people = []\n",
    "                    #tts = gTTS(text=fin_situation, lang='ko')\n",
    "                    #tts.save(\"helloKO.mp3\")                    \n",
    "        i=i+1\n",
    "       # print(\"--------------------------------------------------- people: \", people)\n",
    "       # print(\"--------------------------------------------------- situation: \", situation)\n",
    "       # print(\"--------------------------------------------------- face: \", face_names)\n",
    "        \n",
    "                \n",
    "        \n",
    "\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == 27: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
